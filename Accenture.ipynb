{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24cb1557-9f0d-45bd-aad7-56ca6997b737",
   "metadata": {},
   "outputs": [],
   "source": [
    "#导入必要的包\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import random\n",
    "import time\n",
    "import re\n",
    "import jieba\n",
    "import jieba.analyse as analyse\n",
    "from requests.exceptions import RequestException\n",
    "from wordcloud import WordCloud\n",
    "from PIL import Image\n",
    "import numpy as np   #科学数值计算包，可用来存储和处理大型矩阵\n",
    "import PIL\n",
    "\n",
    "\n",
    "user_Agent = ['Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/61.0.3163.100 Safari/537.36','Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36 OPR/26.0.1656.60', 'Mozilla/5.0 (Windows NT 5.1; U; en; rv:1.8.1) Gecko/20061208 Firefox/2.0.0 Opera 9.50', 'Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; en) Opera 9.50', 'Mozilla/5.0 (Windows NT 6.1; WOW64; rv:34.0) Gecko/20100101 Firefox/34.0', 'Mozilla/5.0 (X11; U; Linux x86_64; zh-CN; rv:1.9.2.10) Gecko/20100922 Ubuntu/10.10 (maverick) Firefox/3.6.10', 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/534.57.2 (KHTML, like Gecko) Version/5.1.7 Safari/534.57.2','Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.71 Safari/537.36', 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.11 (KHTML, like Gecko) Chrome/23.0.1271.64 Safari/537.11', 'Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US) AppleWebKit/534.16 (KHTML, like Gecko) Chrome/10.0.648.133 Safari/534.16', 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/30.0.1599.101 Safari/537.36','Mozilla/5.0 (Windows NT 6.1; WOW64; Trident/7.0; rv:11.0) like Gecko','Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.1 (KHTML, like Gecko) Chrome/21.0.1180.71 Safari/537.1 LBBROWSER','Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; WOW64; Trident/5.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; Media Center PC 6.0; .NET4.0C; .NET4.0E; LBBROWSER)','Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; WOW64; Trident/5.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; Media Center PC 6.0; .NET4.0C; .NET4.0E; QQBrowser/7.0.3698.400)','Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; QQDownload 732; .NET4.0C; .NET4.0E)','Mozilla/5.0 (Windows NT 5.1) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.84 Safari/535.11 SE 2.X MetaSr 1.0','Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; Trident/4.0; SV1; QQDownload 732; .NET4.0C; .NET4.0E; SE 2.X MetaSr 1.0)']\n",
    "jobs = ['数据挖掘', '图像算法工程师', 'java后端', '互联网产品经理']\n",
    "citys = {'北京':'010', '上海':'020', '深圳':'050090', '广州':'050020', '武汉':'170020', '杭州':'070020'}\n",
    "proxy=[ '121.201.38.71:3128',\n",
    "\t   '123.161.16.163:9797',\n",
    "\t   '183.196.170.247:9000',\n",
    "\t   '114.249.119.102:9000',\n",
    "\t   '58.250.21.56:3128',\n",
    "\t   '47.100.171.38:8080',\n",
    "\t   '58.220.95.35:10174',\n",
    "\t   '60.211.218.78:53281',\n",
    "\t\t'110.243.31.66:9999',\n",
    "\t\t'112.64.233.130:9991',\n",
    "\t\t'60.217.64.237:38829',\n",
    "\t\t'125.46.0.62:53281',\n",
    "\t\t'223.82.106.253:3128',\n",
    "\t\t'221.182.31.54:8080',\n",
    "\t\t'58.220.95.34:10174'\n",
    "\t\t]\n",
    "\n",
    "\n",
    "#爬取职位URL\n",
    "def get_url():\n",
    "\tfor job in jobs:\n",
    "\t\tprint(\"职位：\"+job)\n",
    "\t\tfor city,cityid in citys.items():\n",
    "\t\t\tprint(\"城市：\"+city)\n",
    "\t\t\tfor curPage in range(0,3):\n",
    "\t\t\t\ttime.sleep(random.randint(1,2))\n",
    "\t\t\t\t#发起访问请求\n",
    "\t\t\t\turl=\"https://www.liepin.com/zhaopin/?\"+'key='+job+'&dqs='+cityid+'&curPage='+str(curPage)\n",
    "\t\t\t\t\n",
    "\t\t\t\t#返回page类\n",
    "\t\t\t\twhile True:\n",
    "\t\t\t\t\ttry:\n",
    "\t\t\t\t\t\tproxies={ 'http':'http://'+random.choice(proxy),\n",
    "\t\t\t\t\t\t\t\t 'https':'https://'+random.choice(proxy)\n",
    "\t\t\t\t\t\t\t\t}\n",
    "\t\t\t\t\t\theaders={\"User-Agent\":random.choice(user_Agent)}\n",
    "\t\t\t\t\t\tpage=requests.get(url=url,headers=headers,proxies=proxies,timeout=10)\n",
    "\t\t\t\t\t\tpage.encoding=page.apparent_encoding\n",
    "\t\t\t\t\t\tprint(page)\n",
    "\t\t\t\t\t\tbreak\n",
    "\t\t\t\t\texcept RequestException as e:\n",
    "\t\t\t\t\t\t\t print('重新选用ip地址：')\n",
    "\t\t\t\t\t\t\t print(proxies['https'])\n",
    "\t\t\t\t\t\t\t time.sleep(random.randint(1,2))\n",
    "\n",
    "\t\t\t\tprint(page.status_code)\n",
    "\t\t\t\t# 初始化 soup 对象,page.text为爬取到的带有html标签页面\n",
    "\t\t\t\tsoup = BeautifulSoup(page.text,\"html.parser\")\n",
    "\t\t\t\t# 找到<h3>标签，实质是获取所有包含职位名称及链接的标签内容\n",
    "\t\t\t\tsoup = soup.find_all(\"h3\")\n",
    "\t\t\t\t#在每个<h3>中进行抽取链接信息\n",
    "\t\t\t\tfor i in soup:\n",
    "\t\t\t\t\t#有些<h3>标签不包含求职信息，做简要判断\n",
    "\t\t\t\t\tif i.has_attr(\"title\"):\n",
    "\t\t\t\t\t\t#抽取链接内容'\n",
    "\t\t\t\t\t\thref=i.find_all(\"a\")[0][\"href\"]\n",
    "\t\t\t\t\t\tif href[0]=='/':\n",
    "\t\t\t\t\t\t\thref=\"https://www.liepin.com\"+href\n",
    "\t\t\t\t\t\tfilename='url\\\\'+job+'.txt'\n",
    "\t\t\t\t\t\twith open(filename,'a') as file_object:\n",
    "\t\t\t\t\t\t\tfile_object.write(href+'\\n')\n",
    "#爬取职位要求 \n",
    "def get_detail():\n",
    "\tfor job in jobs:\n",
    "\t\tprint(\"职位：\"+job)\n",
    "\t\tfilename='url\\\\'+job+'.txt'\n",
    "\t\twith open(filename,'r') as file_object:\n",
    "\t\t\tlines=file_object.readlines()\n",
    "\t\tfor line in lines:\n",
    "\t\t\turl=line\t\n",
    "\t\t\t#time.sleep(random.randint(1,2))\n",
    "\t\t\t\n",
    "\t\t\twhile True:\n",
    "\t\t\t\ttry:\n",
    "\t\t\t\t\tproxies={ 'http':'http://'+random.choice(proxy),\n",
    "\t\t\t\t\t\t\t 'https':'https://'+random.choice(proxy)\n",
    "\t\t\t\t\t\t\t}\n",
    "\t\t\t\t\theaders={\"User-Agent\":random.choice(user_Agent)}\n",
    "\t\t\t\t\tpage=requests.get(url=url,headers=headers,proxies=proxies,timeout=10)\n",
    "\t\t\t\t\tpage.encoding=page.apparent_encoding\n",
    "\t\t\t\t\tprint(page)\n",
    "\t\t\t\t\tbreak\n",
    "\t\t\t\texcept RequestException as e:\n",
    "\t\t\t\t\t\t print('重新选用ip地址：')\n",
    "\t\t\t\t\t\t print(proxies['https'])\n",
    "\t\t\t\t\t\t# time.sleep(random.randint(1,2))\n",
    "\t\t\n",
    "\t\t\tsoup=BeautifulSoup(page.text,\"html.parser\")\n",
    "\t\t\t#print(soup.title.string)\n",
    "\t\t\ttry:\n",
    "\t\t\t\tsoup=soup.find(name='div',attrs={'class':'content content-word'}).get_text()\n",
    "\t\t\t\tresult=re.search('(要求|资格)：?(.*)',soup,re.S).group(2)\n",
    "\t\t\t\tresult=result.replace(' ','').replace('\\t','').replace('”','').replace('“','').replace(':','').strip()\n",
    "\t\t\t\tif result=='None':\n",
    "\t\t\t\t\tcontinue\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\t#print(result)\n",
    "\t\t\t\t\tfilename='url\\\\'+job+'职业要求'+'.txt'\n",
    "\t\t\t\t\tf=open(filename,'a',encoding='utf-8')\n",
    "\t\t\t\t\tfor x in re.split(r'(?:[；。])',result):\n",
    "\t\t\t\t\t\tif x=='':\n",
    "\t\t\t\t\t\t\tcontinue\n",
    "\t\t\t\t\t\tf.write(x.strip()+'\\n')\n",
    "\t\t\t\t\tf.write('\\n')\n",
    "\t\t\t\t\tf.close()\n",
    "\t\t\t#职位链接失效\n",
    "\t\t\texcept AttributeError as e:\n",
    "\t\t\t\tcontinue\n",
    "\n",
    "\n",
    "def get_keyword():\n",
    "\tfor job in jobs:\n",
    "\t\tfilename='url\\\\'+job+'职业要求'+'.txt'\n",
    "\t\twith open(filename,'r',encoding='utf-8') as f:\n",
    "\t\t\tdata=f.read()\n",
    "\t\t#数据清洗-结巴分词-精确模式\n",
    "\t\tseg_list=jieba.lcut(data,cut_all=False)\n",
    "\t\t#去除停用词,单字\n",
    "\t\tstopword_file=open('url\\\\cn_stopwords.txt','r',encoding='utf-8')\n",
    "\t\tstopword=stopword_file.read().split('\\n')\n",
    "\t\tword_list=[]\n",
    "\t\tword_num=0\n",
    "\t\tfor key in seg_list:\n",
    "\t\t\tif key.strip() not in stopword and len(key.strip())>1:\n",
    "\t\t\t\tword_list.append(key)\n",
    "\t\t\t\tword_num+=1\n",
    "\t\tword=' '.join(word_list)\n",
    "\t\t#关键词提取-基于TF-IDF关键字提取\n",
    "\t\tfilename='url\\\\'+job+'职业要求'+'_keyword.txt'\n",
    "\t\tkeyword_file=open(filename,'w',encoding='utf-8')\n",
    "\t\tfor key,value in analyse.extract_tags(word,topK=(int)(200),withWeight=True):\n",
    "\t\t\tkeyword_file.write(key+' '+str(value)+'\\n')\n",
    "\t\tkeyword_file.close()\n",
    "\t\tstopword_file.close()\n",
    "\n",
    "\n",
    "def get_wordcloud():\n",
    "\tfor job in jobs:\n",
    "\t\tfilename='url\\\\'+job+'职业要求'+'_keyword.txt'\n",
    "\t\tkeyword_file=open(filename,'r',encoding='utf-8')\n",
    "\t\tkeywords={}\n",
    "\t\tfor line in keyword_file:\n",
    "\t\t\tkey_frequencies=line.strip().split(' ')\n",
    "\t\t\tkeywords[key_frequencies[0]]=float(key_frequencies[1]) \n",
    "\t\t#自定义背景图片\t\t\n",
    "\t\t'''image1=PIL.Image.open(r'url\\\\paojie.jpg')\n",
    "\t\tgraph=np.array(image1)'''\n",
    "\t\t#设置词云背景，大小，字体等\n",
    "\t\twordcloud=WordCloud(\n",
    "\t\t\tfont_path=\"C:\\\\Windows\\\\Fonts\\\\simfang.ttf\",\n",
    "\t\t\tbackground_color='white',\n",
    "\t\t\twidth=1000,\n",
    "\t\t\theight=800,\n",
    "\t\t\t#mask=graph,\n",
    "\t\t\t#color_func=lambda *args, **kwargs: \"black\"  #所有字体为黑色\n",
    "\t\t\t)\n",
    "\t\t#生成词云\n",
    "\t\twordcloud.generate_from_frequencies(keywords)\n",
    "\t\t#词云保存\n",
    "\t\tfilename='url\\\\'+job+'_wordcloud.png'\n",
    "\t\twordcloud.to_file(filename)\n",
    "\n",
    "\n",
    "def menu():\n",
    "\tmessage = input(\"url爬取请输入1\\n职业详情爬取请输入2\\n数据清洗请输入3\\n词云生成请按4\\n\")\n",
    "\tif message=='1':\n",
    "\t\tget_url()\n",
    "\t\tmenu()\n",
    "\telif message=='2':\n",
    "\t\tget_detail()\n",
    "\t\tmenu()\n",
    "\telif message=='3':\n",
    "\t\tget_keyword()\n",
    "\t\tmenu()\n",
    "\telif message=='4':\n",
    "\t\tget_wordcloud()\n",
    "\t\tmenu()\n",
    "\telse:\n",
    "\t\t return 0\n",
    "\n",
    "menu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7ee596-efd7-46e7-9c93-0a68c08b7899",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "url= 'https://www.accenture.com/hk-en/careers/jobsearch?jk=&sb=1&vw=0&is_rj=0&pg=1'\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/113.0.0.0 Safari/537.36'\n",
    "}\n",
    "\n",
    "response = requests.get(url,headers)\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "title = soup.title.string\n",
    "\n",
    "print(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b21ddb-493b-4baf-a7c3-35c3122cc6a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b168d892-d1f3-47eb-a6e0-e4792c4cb989",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
